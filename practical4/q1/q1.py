# imports
import os
import glob
import nltk
from nltk.corpus import stopwords
import math



# GENERAL FUNCTIONS
# directory changes
def go_a_to_tweets():
	os.chdir("Tweets")

def go_tweets_to_a():
	os.chdir("../")

def go_a_to_stop_removed():
	os.chdir("Tweets_no_stop")

def go_stop_removed_to_a():
	os.chdir("../")



# TOKENIZE FUNCTIONS
def tokenize_text_file(file_name):
	"""
	This function generates a token list based on a file input
	"""
	# open the text file that will be tokenized
	with open(file_name, "r") as source:
		# return the tokens generated by reading the source file
		return nltk.word_tokenize(source.read())

def tokenize_text_file_remove_stop_words(file_name):
	"""
	This function generates a token list from an input file and removes stop words
	"""
	# generate the tokens
	tokens = tokenize_text_file(file_name)
	# generate the stop words
	stop_words = set(stopwords.words('english'))
	# return
	return [t for t in tokens if t not in stop_words]



# TF SCORES
def tf_boolean(file_name):
	# data
	token_list = tokenize_text_file_remove_stop_words(file_name)
	frequency_dict = dict()
	# populate frequency_dict
	for token in token_list:
		if token in frequency_dict:
			frequency_dict[token] += 1
		else:
			frequency_dict[token] = 1
	# return frequency_dict
	return frequency_dict



# MATRIX
def get_file_names():
	# file_name_list
	file_name_list = list()
	# iterate over files
	for file_name in glob.glob("*tweet"):
		file_name_list.append(file_name)
	# return file_name_list
	return file_name_list

def get_terms():
	# term_list
	term_dict = dict()
	# iterate over the files
	for file_name in glob.glob("*tweet"):
		# generate a tf_boolean_dict
		tf_boolean_dict = tf_boolean(file_name)
		# iterate over the terms to populate term_dict
		for term in tf_boolean_dict:
			if term in term_dict:
				if file_name in term_dict[term]:
					term_dict[term][file_name] += 1
				else:
					term_dict[term][file_name] = 1
			else:
				term_dict[term] = dict()
				term_dict[term][file_name] = 1
	# return
	return term_dict

def create_tf_matrix():
	# data
	file_name_list = get_file_names()
	file_name_list.insert(0, "")
	term_dict = get_terms()
	# term_list
	term_list = list() # a list of lists
	for term in term_dict:
		temp_list = [term]
		for f in file_name_list[1::1]:
			if f in term_dict[term]:
				temp_list.append(term_dict[term][f])
			else:
				temp_list.append(0)
		term_list.append(temp_list)
	# modify term_list to be the final list
	term_list.insert(0, file_name_list)
	# return 
	return term_list


def print_matrix(matrix):
	for array in matrix:
		print(" ".join(["{:>16}".format(item) for item in array]))



# IDF SCORES
# idf numerator
def get_idf_numerator():
	# return number of files sourced
	return round(float(len(get_file_names())), 2)

# idf denominator
def get_idf_denominator(term, term_dict):
	return round(float(1 + len(term_dict[term])), 2) # 1 to void zero denominator

# idf
def get_idf(term, term_dict, base, idf_numerator):
	idf_denominator = get_idf_denominator(term, term_dict)
	return round(float(math.log(idf_numerator/idf_denominator, base)), 2)

# idf_boolean
def td_idf_boolean(term_dict, base):
	# data
	idf_numerator = get_idf_numerator()
	# create the return object
	idf_boolean_dict = term_dict
	# populate the return object
	for term in idf_boolean_dict:
		# data
		idf_denominator = get_idf_denominator(term, term_dict)
		idf = get_idf(term, term_dict, base, idf_numerator)
		for file_name in idf_boolean_dict[term]:
			idf_boolean_dict[term][file_name] *= idf
	# return
	return idf_boolean_dict