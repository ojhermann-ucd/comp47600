# imports
import os
import glob
import nltk
from nltk.corpus import stopwords



# GENERAL FUNCTIONS
# directory changes
def go_a_to_tweets():
	os.chdir("Tweets")

def go_tweets_to_a():
	os.chdir("../")

def go_a_to_stop_removed():
	os.chdir("Tweets_no_stop")

def go_stop_removed_to_a():
	os.chdir("../")



# TOKENIZE FUNCTIONS
def tokenize_text_file(file_name):
	"""
	This function generates a token list based on a file input
	"""
	# open the text file that will be tokenized
	with open(file_name, "r") as source:
		# return the tokens generated by reading the source file
		return nltk.word_tokenize(source.read())

def tokenize_text_file_remove_stop_words(file_name):
	"""
	This function generates a token list from an input file and removes stop words
	"""
	# generate the tokens
	tokens = tokenize_text_file(file_name)
	# generate the stop words
	stop_words = set(stopwords.words('english'))
	# return
	return [t for t in tokens if t not in stop_words]



# TF SCORES
def tf_frequency_no_stop_words(file_name):
	# data
	token_list = tokenize_text_file_remove_stop_words(file_name)
	frequency_dict = dict()
	# populate frequency_dict
	for token in token_list:
		if token in frequency_dict:
			frequency_dict[token] += 1
		else:
			frequency_dict[token] = 1
	# return frequency_dict
	return frequency_dict

def tf_boolean(file_name):
	# data
	frequency_dict = tf_frequency_no_stop_words
	tf_boolean_dict = dict()
	total_tokens = 0
	for token in frequency_dict:
		total_tokens += frequency_dict[token]
	# populate tf_boolean_dict
	for token in frequency_dict:
		tf_boolean_dict[token] = round(float(frequency_dict[token] / total_tokens), 2)
	# return tf_boolean_dict
	return tf_boolean_dict