# imports
import os
import glob
import nltk
from nltk.corpus import stopwords



# GENERAL FUNCTIONS
# directory changes
def go_a_to_tweets():
	os.chdir("Tweets")

def go_tweets_to_a():
	os.chdir("../")

def go_a_to_stop_removed():
	os.chdir("Tweets_no_stop")

def go_stop_removed_to_a():
	os.chdir("../")



# TOKENIZE FUNCTIONS
def tokenize_text_file(file_name):
	"""
	This function generates a token list based on a file input
	"""
	# open the text file that will be tokenized
	with open(file_name, "r") as source:
		# return the tokens generated by reading the source file
		return nltk.word_tokenize(source.read())

def tokenize_text_file_remove_stop_words(file_name):
	"""
	This function generates a token list from an input file and removes stop words
	"""
	# generate the tokens
	tokens = tokenize_text_file(file_name)
	# generate the stop words
	stop_words = set(stopwords.words('english'))
	# return
	return [t for t in tokens if t not in stop_words]