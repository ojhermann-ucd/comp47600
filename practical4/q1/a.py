# imports
import os
import glob
import nltk
from nltk.corpus import stopwords



# DIRECTORY CHANGE FUNCTIONS
def go_a_to_tweets():
	os.chdir("Tweets")

def go_tweets_to_a():
	os.chdir("../")

def go_a_to_stop_removed():
	os.chdir("Tweets_no_stop")

def go_stop_removed_to_a():
	os.chdir("../")



# TOKENIZE FUNCTIONS
def tokenize_text_file(file_name):
	"""
	This function generates a token list based on a file input
	"""
	# open the text file that will be tokenized
	with open(file_name, "r") as source:
		# return the tokens generated by reading the source file
		return nltk.word_tokenize(source.read())

def tokenize_text_file_remove_stop_words(file_name):
	"""
	This function generates a token list from an input file and removes stop words
	"""
	# generate the tokens
	tokens = tokenize_text_file(file_name)
	# generate the stop words
	stop_words = set(stopwords.words('english'))
	# return
	return [t for t in tokens if t not in stop_words]




if __name__ == "__main__":
	
	# got the source files
	go_a_to_tweets()

	# tokenize and remove stop words for each file
	for file_name in glob.glob("*.tweet"):
		# create the token list
		token_list = tokenize_text_file_remove_stop_words(file_name)
		# convert to string
		token_string = " ".join(token_list)
		# go to the folder for modifed tweets
		go_tweets_to_a()
		go_a_to_stop_removed()
		# save the new tokens with stop words removed
		with open(file_name, "w") as destination:
			destination.write(token_string)
		# go to the source of the tweets
		go_stop_removed_to_a()
		go_a_to_tweets()

	# return to the starting directory
	go_tweets_to_a()


