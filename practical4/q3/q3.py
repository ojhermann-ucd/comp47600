#imports
# imports
import os
import glob
import nltk
from nltk.corpus import stopwords
from nltk.probability import FreqDist # http://www.nltk.org/api/nltk.html?highlight=freqdist
import math



# TOKENIZE FUNCTIONS
def tokenize_text_file(file_name):
	"""
	This function generates a token list based on a file input
	"""
	# open the text file that will be tokenized
	with open(file_name, "r") as source:
		# return the tokens generated by reading the source file
		return nltk.word_tokenize(source.read())

def tokenize_text_file_remove_stop_words(file_name):
	"""
	This function generates a token list from an input file and removes stop words
	"""
	# generate the tokens
	tokens = tokenize_text_file(file_name)
	# generate the stop words
	stop_words = set(stopwords.words('english'))
	# return
	return [t for t in tokens if t not in stop_words]



# ENTROPY FUNCTION
def entropy(file_name):
	# data
	tokens = tokenize_text_file_remove_stop_words(file_name) # create the tokens
	frequencies = FreqDist(tokens) # generate the frequencies
	probabilities = [frequencies.freq(p) for p in frequencies] # calculate the probabilities
	return round(-sum(p * math.log(p, 2) for p in probabilities),2) # return the measure of entropy



if __name__ == '__main__':
	
	# INDIVIDUAL ENTROPY
	print("")
	print("Individual Entropy: Random")
	for file_name in glob.glob("*random"):
		print("{}: {}".format(file_name, entropy(file_name)))

	print("")
	print("Individual Entropy: Spam")
	for file_name in glob.glob("*spam"):
		print("{}: {}".format(file_name, entropy(file_name)))	